framework:
  title: Comprehensive Prompt Engineering Framework (Aligned with Boonstra, 2025 PDF)
  core_techniques:
    must_include:
      - Zero-shot Prompting
      - Few-shot Prompting
      - System Prompting
      - Role Prompting
      - Contextual Prompting
      - Step-back Prompting
      - Chain of Thought
      - Self-Consistency
      - Tree of Thoughts
      - ReAct

  layers:
    - name: Core Foundation
      techniques:
        - name: Zero-shot Prompting
          purpose: Direct instruction without examples [cite: 87]
          notes:
            - Uses modelâ€™s pre-trained knowledge [cite: 84]
            - Foundation for many other techniques [cite: 87]
        - name: Few-shot Prompting
          purpose: Learning from examples provided in the prompt [cite: 101, 103]
          subtypes:
            - One-shot: Single example [cite: 105]
            - Few-shot: Multiple examples (typically 3-5 or more, depending on task and model limits) [cite: 107, 111, 112]
          notes:
            - Builds context and demonstrates desired output structure or pattern [cite: 104]
            - Crucial best practice: "Provide examples" [cite: 351]
        - name: System Prompting
          purpose: Behavior configuration and defining overall context/purpose for the LLM [cite: 124, 131]
          notes:
            - Controls tone, behavior, rules, output format (e.g., return label in uppercase, use JSON) [cite: 136, 142, 140]
            - Can be persistent across interactions.
            - Useful for safety and toxicity control [cite: 149]

    - name: Context Engineering
      techniques:
        - name: Role Prompting
          purpose: Assigns a specific character, persona, or identity for the LLM to adopt [cite: 127, 151]
          example: "Act as a travel guide..." [cite: 155]
          notes:
            - Influences tone, style, and focused expertise [cite: 161]
        - name: Contextual Prompting
          purpose: Provides specific details, background information, or situational constraints relevant to the current task [cite: 125, 132]
          notes:
            - Helps model understand nuances and tailor responses [cite: 126]
            - Can be dynamic and task-specific [cite: 133]

    - name: Reasoning Architecture
      techniques:
        - name: Step-back Prompting
          purpose: Improves reasoning by prompting the LLM to first consider a general question or abstract concepts related to the specific task [cite: 169, 170]
          notes:
            - Activates relevant background knowledge and principles before solving [cite: 171]
            - Useful for critical thinking and mitigating biases [cite: 173, 175]
        - name: Chain of Thought (CoT)
          purpose: Improves reasoning by prompting the LLM to generate intermediate steps leading to the final answer [cite: 190, 191]
          pattern: "Let's think step by step." (example of zero-shot CoT trigger) [cite: 205]
          notes:
            - Enhances interpretability and can improve robustness [cite: 194, 196]
            - Can be combined with few-shot prompting for complex tasks [cite: 192]
            - Best practice: Set temperature to 0 for CoT when a single correct reasoning path is expected [cite: 438, 439]
            - Best practice: Ensure the answer follows the reasoning steps [cite: 436]
        - name: Self-Consistency
          purpose: Enhances reasoning reliability by generating multiple diverse reasoning paths (e.g., via CoT with higher temperature) and selecting the most consistent answer by majority vote [cite: 224]
          notes:
            - Improves accuracy and coherence over simple greedy decoding in CoT [cite: 224]
            - Involves multiple generation steps, increasing cost [cite: 224]
        - name: Tree of Thoughts (ToT)
          purpose: Generalizes CoT by allowing LLMs to explore multiple different reasoning paths (thoughts) simultaneously in a tree structure [cite: 240, 241]
          notes:
            - Model can branch out, evaluate intermediate thoughts, and prune less promising paths [cite: 245]
            - Suited for complex tasks requiring exploration and deliberate problem-solving [cite: 243, 247]
        - name: ReAct Framework (Reason & Act)
          purpose: Enables LLMs to solve complex tasks by synergizing reasoning with the ability to take actions using external tools (e.g., search, code interpreter) [cite: 247, 248]
          pattern: Thought -> Action (tool use) -> Observation -> Repeat until solution [cite: 251, 254]
          notes:
            - Mimics human problem-solving by combining verbal reasoning with information gathering [cite: 249]
            - Requires careful setup and handling of previous prompts/responses [cite: 273]

    - name: Output Engineering
      techniques:
        - name: Structured Output Specification
          purpose: Guiding the LLM to produce output in a predictable, parseable format.
          formats: [JSON, XML, Markdown, Table, Custom Formats] [cite: 412]
          notes:
            - "Be specific about the output" is a key best practice [cite: 364]
            - Prompting for JSON can enforce structure and limit hallucinations [cite: 414]
            - Benefits of JSON: consistent style, data focus, type awareness, sortability, relationship awareness [cite: 413, 414]
        - name: JSON Schema Guidance (for Input & Output)
          purpose: Using JSON Schema to define the expected structure and data types of JSON, for both input comprehension and output generation.
          notes:
            - For input: Helps LLM understand data structure, focus on relevant info, and establish relationships[cite: 425, 426, 427, 428]. (See Snippets 5 & 6 for input schema example).
            - For output: Guides the LLM to generate JSON conforming to a specified schema [cite: 145] (See Table 4 for output schema example).
        - name: JSON Repair
          purpose: Handling potentially malformed JSON output from LLMs, especially due to truncation.
          tools:
            - `json-repair` library (Python) [cite: 421]
          notes:
            - Useful when JSON output is abruptly cut off due to token limits [cite: 419, 420]

    - name: Implementation Strategy
      techniques:
        - name: LLM Output Configuration
          purpose: Adjusting model parameters to control output generation characteristics.
          parameters:
            - Output Length (Max Tokens): Controls number of tokens generated; impacts cost, speed, and potential truncation [cite: 32, 33, 35]
            - Temperature: Controls randomness. Lower for deterministic, higher for diverse/creative output[cite: 42, 43]. A temperature of 0 is deterministic (greedy decoding)[cite: 43].
            - Top-K: Restricts sampling to K most likely tokens[cite: 51, 53].
            - Top-P (Nucleus Sampling): Restricts sampling to tokens whose cumulative probability exceeds P[cite: 51, 57].
          notes:
            - These settings interact and optimal values depend on the task[cite: 60, 61].
            - Extreme settings can render others irrelevant (e.g., Temp 0 or Top-K 1)[cite: 67, 69, 72].
            - Be aware of issues like repetition loops at very low or high temperatures[cite: 77, 78, 79, 80].
        - name: Variables in Prompts
          purpose: Making prompts dynamic and reusable by using placeholders for changing inputs[cite: 390].
          example: "Tell me a fact about the city: {city}" [cite: 395]
          notes:
            - Saves effort, avoids repetition, useful for application integration [cite: 393, 394]
        - name: Documentation & Iteration Methods
          purpose: Systematically tracking prompt development and results for learning and reproducibility[cite: 440].
          methods:
            - Detailed tracking sheet (e.g., Google Sheet with fields like Name, Goal, Model, Configs, Prompt, Output, Feedback, Version - as per Table 21) [cite: 445, 447, 458]
            - Saving prompts in tools (e.g., Vertex AI Studio) and linking to documentation [cite: 409, 448]
            - Version control for prompts.
            - Documenting RAG system specifics (query, chunk settings) if applicable [cite: 450]
          notes:
            - Prompt engineering is an iterative process: craft, test, analyze, document, refine [cite: 11, 454, 455]
        - name: Prompt Debugging & Review
          purpose: Identifying and fixing issues in prompts or generated code/text.
          methods:
            - Error analysis from LLM outputs or tracebacks [cite: 332]
            - Using LLMs to explain or debug code snippets [cite: 305, 332]
            - Iterative refinement based on performance [cite: 455]
            - Include edge cases in few-shot examples for robustness [cite: 121]
        - name: Model-Specific Considerations & Updates
          purpose: Tailoring prompts to specific model capabilities and adapting to model changes[cite: 27].
          notes:
            - Prompts may need optimization for different models or model versions [cite: 27, 408]
            - Stay updated on model architecture, training data, and new features [cite: 408]
            - Re-test prompts with model updates [cite: 455]
        - name: Instructions over Constraints
          purpose: Guiding the model by telling it what to do (instructions) rather than what not to do (constraints)[cite: 370, 371, 373].
          notes:
            - Positive instructions can be more effective and flexible[cite: 375, 378].
            - Constraints are valuable for safety, preventing harmful content, or strict output formats[cite: 379, 380].
            - Prioritize instructions; use constraints when necessary[cite: 386].

    - name: Advanced Integration
      techniques:
        - name: Combined Prompting Techniques
          purpose: Leveraging multiple prompting strategies within a single interaction for complex tasks.
          pattern_example: |
            SYSTEM: [Behavior rules, overall purpose] [cite: 124]
            ROLE: [Specific persona, e.g., "You are an expert physicist"] [cite: 127]
            CONTEXT: [Background: "The experiment failed due to X..."] [cite: 125]
            EXAMPLES: [Few-shot: Example 1 problem & solution...] [cite: 101]
            REASONING_TRIGGER: [Chain of Thought: "Explain your reasoning step-by-step."] [cite: 190]
            TASK: [Specific instruction: "Propose three alternative solutions..."]
            OUTPUT_FORMAT: [Structure: "Provide your answer in JSON format with keys 'solution_name', 'rationale', 'potential_issues'."] [cite: 142]
          notes:
            - System, contextual, and role prompting can overlap and be used together [cite: 129]
        - name: RAG (Retrieval Augmented Generation) + Prompting
          purpose: Combining external knowledge retrieval with LLM's reasoning and generation capabilities.
          usage: Retrieved context is injected into the prompt, which then might use CoT, ToT, etc., for processing[cite: 450].
        - name: Multimodal Prompting
          purpose: Using multiple input formats (text, images, audio, code) to guide an LLM[cite: 347, 348].
          notes:
            - Depends on the model's capabilities[cite: 348].

    - name: Automation & Optimization Strategies
      techniques:
        - name: Automatic Prompt Engineering (APE)
          purpose: Using LLMs to generate, evaluate, and refine prompts for specific tasks[cite: 277, 278].
          methods:
            - Prompt an LLM to generate prompt variants for a task[cite: 282].
            - Evaluate candidates using metrics (e.g., BLEU, ROUGE) or model-based evaluation[cite: 286].
            - Select the highest-scoring or best-performing prompt[cite: 288].
        - name: Prompt Design Principles
          purpose: Crafting effective and efficient prompts.
          principles:
            - Design with simplicity: Clear, concise, easy to understand[cite: 356, 357].
            - Be specific about the output: Guide the model clearly on desired format, length, content[cite: 364, 365].
            - Experiment with input formats and writing styles: Questions, statements, instructions can yield different results[cite: 397, 399].
            - For few-shot classification, mix up class order in examples: Avoids overfitting to example order[cite: 402, 403, 404].
            - Experiment together with other prompt engineers: Leverage diverse perspectives[cite: 433].
        - name: Prompt Guardrails & Safety
          purpose: Ensuring responsible and safe LLM outputs.
          methods:
            - System prompts for respectful behavior or avoiding certain topics[cite: 149, 150].
            - Using instructions to guide behavior and constraints to prevent harmful content[cite: 380, 386].

    - name: Application-Specific Patterns
      categories:
        - name: Code Applications
          combo_description: For generating, explaining, translating, or debugging code.
          relevant_techniques: [System Prompting, Role Prompting, Few-shot (with code examples), Chain of Thought (for logic breakdown), Structured Output (code blocks), LLM for Debugging]
        - name: Content Creation & Summarization
          combo_description: For drafting articles, stories, summaries, or marketing copy.
          relevant_techniques: [Role Prompting, Contextual Prompting, Few-shot (style examples), Step-back (for broader themes), Specific Output Instructions (length, paragraphs), System Prompting (for tone)]
        - name: Complex Reasoning & Decision Support
          combo_description: For tasks requiring analysis, problem-solving, or evaluation.
          relevant_techniques: [Chain of Thought, Self-Consistency, Tree of Thoughts, Step-back Prompting, ReAct, Contextual Prompting (with data), Structured Output (e.g., JSON for comparisons)]
        - name: Information Extraction & Classification
          combo_description: For parsing data from text or categorizing inputs.
          relevant_techniques: [Few-shot (with examples of extraction/classification), System Prompting (for output format/labels), JSON Output, Contextual Prompting (with source text)] [cite: 91, 99, 113, 144]

  meta_framework_summary:
    name: Unified Prompting Strategy (Iterative & Layered Approach)
    description: A comprehensive approach starting with foundational techniques, layering context and reasoning, structuring output, and continuously enhancing and optimizing through iterative best practices and advanced integrations.
    steps_overview:
      - 1_Foundation: Start with clear base instructions (Zero/Few-shot).
      - 2_Contextualization: Provide necessary background, role, and system-level guidance.
      - 3_Reasoning: Employ appropriate reasoning architecture if the task is complex.
      - 4_Output_Structuring: Clearly define the desired output format.
      - 5_Implementation_Tuning: Configure LLM parameters and use implementation best practices (variables, documentation).
      - 6_Iteration_Optimization: Continuously test, analyze, refine, and explore automation or advanced integrations.

  meta_prompt_pattern_example: |
    # Phase 1: System & Role Setup (Behavior, Persona)
    SYSTEM_PROMPT: "You are a helpful and expert {{domain_expert}} assistant. Your responses should be {{tone_adjective}} and strictly follow formatting guidelines. Avoid discussing {{restricted_topic}}." [cite: 124, 149]
    ROLE_PROMPT: "Assume the persona of a {{specific_role}}, for example, a senior software architect reviewing code." [cite: 127]

    # Phase 2: Context & Examples (Background, Demonstrations)
    CONTEXT: "The user is working on {{project_details}}. They have encountered {{specific_problem}}. Previous attempts involved {{previous_attempts}}." [cite: 125]
    FEW_SHOT_EXAMPLES: (Optional, if beneficial)
      "Input: {{example1_input}}
       Output: {{example1_output_demonstrating_reasoning_and_format}}" [cite: 101, 351]

    # Phase 3: Task & Reasoning Instructions (Core Request, How to Think)
    USER_QUERY_OR_TASK: "{{specific_question_or_task_for_the_LLM}}"
    REASONING_GUIDANCE: (If complex, e.g., CoT) "Let's think step by step to arrive at the solution. First, analyze X, then consider Y..." [cite: 190, 205]

    # Phase 4: Output Specification (Format, Constraints)
    OUTPUT_INSTRUCTIONS: "Please provide your answer in valid JSON format according to the following schema: {{json_schema_definition}}. Ensure all {{required_elements}} are present. The summary should be approximately {{length_constraint}}." [cite: 142, 364, 412]

    # Phase 5: Validation/Self-Correction Cue (Optional)
    VALIDATION_CUE: "Before finalizing, double-check that your response addresses all parts of the query and adheres to the specified output format." [Implied by self-consistency and iterative refinement]